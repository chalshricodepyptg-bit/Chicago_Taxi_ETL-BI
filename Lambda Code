import boto3
import urllib3
import json
import io
import csv

s3 = boto3.client("s3")
http = urllib3.PoolManager()

RAW_BUCKET = "chicago-taxi-raw-2024"
RAW_PREFIX = "2024/01/"
LIMIT = 2000

MONTH_START = "2024-01-01"
MONTH_END   = "2024-02-01"

def lambda_handler(event, context):
    offset = event.get("offset", 0)

    # Correct dataset for 2024+ (ajtu-isnz)
    url = (
        "https://data.cityofchicago.org/resource/ajtu-isnz.json"
        f"?$where=trip_start_timestamp >= '{MONTH_START}' AND trip_start_timestamp < '{MONTH_END}'"
        f"&$limit={LIMIT}&$offset={offset}"
    )

    response = http.request("GET", url)
    rows = json.loads(response.data.decode("utf-8"))

    # If no data left, finish
    if not rows:
        return {"done": True, "offset": offset}

    # ---------------------------
    # MERGE ALL FIELDNAMES SAFELY
    # ---------------------------
    fieldnames = set()
    for row in rows:
        fieldnames.update(row.keys())
    fieldnames = list(fieldnames)

    # ---------------------------
    # WRITE CSV WITH FULL HEADER
    # ---------------------------
    csv_buffer = io.StringIO()
    writer = csv.DictWriter(csv_buffer, fieldnames=fieldnames)
    writer.writeheader()

    for row in rows:
        # Fill missing fields with empty strings
        safe_row = {field: row.get(field, "") for field in fieldnames}
        writer.writerow(safe_row)

    key = f"{RAW_PREFIX}batch_{offset}.csv"

    s3.put_object(
        Bucket=RAW_BUCKET,
        Key=key,
        Body=csv_buffer.getvalue().encode("utf-8")
    )

    return {
        "done": False,
        "next_offset": offset + LIMIT,
        "file_saved": key
    }
